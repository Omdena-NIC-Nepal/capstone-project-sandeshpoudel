{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c09c963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Severe floods devastate villages in Western Nepal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Government launches reforestation project in C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Drought conditions worsen affecting crops in T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Researchers warn of accelerated glacial melt i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Community builds flood barriers to protect far...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline\n",
       "0  Severe floods devastate villages in Western Nepal\n",
       "1  Government launches reforestation project in C...\n",
       "2  Drought conditions worsen affecting crops in T...\n",
       "3  Researchers warn of accelerated glacial melt i...\n",
       "4  Community builds flood barriers to protect far..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "news_data = {\n",
    "    'headline': [\n",
    "        \"Severe floods devastate villages in Western Nepal\",\n",
    "        \"Government launches reforestation project in Central Nepal\",\n",
    "        \"Drought conditions worsen affecting crops in Terai\",\n",
    "        \"Researchers warn of accelerated glacial melt in Himalayas\",\n",
    "        \"Community builds flood barriers to protect farmland\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "news_df = pd.DataFrame(news_data)\n",
    "news_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a69ce20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK version: 3.9.1\n",
      "sklearn version: 1.6.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download stopwords if not already done\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Check versions\n",
    "import sklearn\n",
    "import nltk\n",
    "print(\"NLTK version:\", nltk.__version__)\n",
    "print(\"sklearn version:\", sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef7a0e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Heavy rainfall and flooding in Nepal have incr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Glacial melt is accelerating, posing severe ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Recent policies aim to mitigate deforestation ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_ID                                               Text\n",
       "0            1  Heavy rainfall and flooding in Nepal have incr...\n",
       "1            2  Glacial melt is accelerating, posing severe ri...\n",
       "2            3  Recent policies aim to mitigate deforestation ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample corpus simulating climate change reports/news\n",
    "data = {\n",
    "    'Document_ID': [1, 2, 3],\n",
    "    'Text': [\n",
    "        \"Heavy rainfall and flooding in Nepal have increased dramatically due to climate change.\",\n",
    "        \"Glacial melt is accelerating, posing severe risks to agriculture and local communities.\",\n",
    "        \"Recent policies aim to mitigate deforestation and reduce greenhouse gas emissions.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "text_df = pd.DataFrame(data)\n",
    "text_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905f0470",
   "metadata": {},
   "source": [
    "Text Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8560ad24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Manually load the tokenizer once to avoid further lookup errors\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "tokenizer = PunktSentenceTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8e357d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "text_df['Tokens'] = text_df['Clean_Text'].apply(lambda x: treebank_tokenizer.tokenize(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdd81b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Text</th>\n",
       "      <th>Clean_Text</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Tokens_NoStop</th>\n",
       "      <th>Lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/2001</td>\n",
       "      <td>Forest loss may increase due to excessive rain...</td>\n",
       "      <td>forest loss may increase due to excessive rain...</td>\n",
       "      <td>[forest, loss, may, increase, due, to, excessi...</td>\n",
       "      <td>[forest, loss, may, increase, due, excessive, ...</td>\n",
       "      <td>[forest, loss, may, increase, due, excessive, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5/15/2005</td>\n",
       "      <td>Deforestation is slowing down in the western r...</td>\n",
       "      <td>deforestation is slowing down in the western r...</td>\n",
       "      <td>[deforestation, is, slowing, down, in, the, we...</td>\n",
       "      <td>[deforestation, slowing, western, region]</td>\n",
       "      <td>[deforestation, slowing, western, region]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7/20/2010</td>\n",
       "      <td>Community forestry efforts have reduced tree c...</td>\n",
       "      <td>community forestry efforts have reduced tree c...</td>\n",
       "      <td>[community, forestry, efforts, have, reduced, ...</td>\n",
       "      <td>[community, forestry, efforts, reduced, tree, ...</td>\n",
       "      <td>[community, forestry, effort, reduced, tree, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9/10/2015</td>\n",
       "      <td>Increased awareness campaigns are helping to p...</td>\n",
       "      <td>increased awareness campaigns are helping to p...</td>\n",
       "      <td>[increased, awareness, campaigns, are, helping...</td>\n",
       "      <td>[increased, awareness, campaigns, helping, pre...</td>\n",
       "      <td>[increased, awareness, campaign, helping, prev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/5/2020</td>\n",
       "      <td>Recent data shows fluctuations in precipitatio...</td>\n",
       "      <td>recent data shows fluctuations in precipitatio...</td>\n",
       "      <td>[recent, data, shows, fluctuations, in, precip...</td>\n",
       "      <td>[recent, data, shows, fluctuations, precipitat...</td>\n",
       "      <td>[recent, data, show, fluctuation, precipitatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date                                               Text  \\\n",
       "0   1/1/2001  Forest loss may increase due to excessive rain...   \n",
       "1  5/15/2005  Deforestation is slowing down in the western r...   \n",
       "2  7/20/2010  Community forestry efforts have reduced tree c...   \n",
       "3  9/10/2015  Increased awareness campaigns are helping to p...   \n",
       "4  12/5/2020  Recent data shows fluctuations in precipitatio...   \n",
       "\n",
       "                                          Clean_Text  \\\n",
       "0  forest loss may increase due to excessive rain...   \n",
       "1  deforestation is slowing down in the western r...   \n",
       "2  community forestry efforts have reduced tree c...   \n",
       "3  increased awareness campaigns are helping to p...   \n",
       "4  recent data shows fluctuations in precipitatio...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [forest, loss, may, increase, due, to, excessi...   \n",
       "1  [deforestation, is, slowing, down, in, the, we...   \n",
       "2  [community, forestry, efforts, have, reduced, ...   \n",
       "3  [increased, awareness, campaigns, are, helping...   \n",
       "4  [recent, data, shows, fluctuations, in, precip...   \n",
       "\n",
       "                                       Tokens_NoStop  \\\n",
       "0  [forest, loss, may, increase, due, excessive, ...   \n",
       "1          [deforestation, slowing, western, region]   \n",
       "2  [community, forestry, efforts, reduced, tree, ...   \n",
       "3  [increased, awareness, campaigns, helping, pre...   \n",
       "4  [recent, data, shows, fluctuations, precipitat...   \n",
       "\n",
       "                                              Lemmas  \n",
       "0  [forest, loss, may, increase, due, excessive, ...  \n",
       "1          [deforestation, slowing, western, region]  \n",
       "2  [community, forestry, effort, reduced, tree, c...  \n",
       "3  [increased, awareness, campaign, helping, prev...  \n",
       "4  [recent, data, show, fluctuation, precipitatio...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Read your text data (assuming your CSV has columns \"Date\" and \"Text\")\n",
    "text_df = pd.read_csv('text_data.csv')\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower())\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "text_df['Clean_Text'] = text_df['Text'].apply(clean_text)\n",
    "\n",
    "# Tokenization using TreebankWordTokenizer (avoids punkt errors)\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text_df['Tokens'] = text_df['Clean_Text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text_df['Tokens_NoStop'] = text_df['Tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text_df['Lemmas'] = text_df['Tokens_NoStop'].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "# Preview the result\n",
    "text_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4292c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>TFIDF_and</th>\n",
       "      <th>TFIDF_are</th>\n",
       "      <th>TFIDF_awareness</th>\n",
       "      <th>TFIDF_campaigns</th>\n",
       "      <th>TFIDF_community</th>\n",
       "      <th>TFIDF_cover</th>\n",
       "      <th>TFIDF_data</th>\n",
       "      <th>TFIDF_deforestation</th>\n",
       "      <th>TFIDF_down</th>\n",
       "      <th>...</th>\n",
       "      <th>TFIDF_rainfall</th>\n",
       "      <th>TFIDF_recent</th>\n",
       "      <th>TFIDF_reduced</th>\n",
       "      <th>TFIDF_region</th>\n",
       "      <th>TFIDF_shows</th>\n",
       "      <th>TFIDF_slowing</th>\n",
       "      <th>TFIDF_the</th>\n",
       "      <th>TFIDF_to</th>\n",
       "      <th>TFIDF_tree</th>\n",
       "      <th>TFIDF_western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/2001</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388684</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5/15/2005</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.361529</td>\n",
       "      <td>0.361529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361529</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.361529</td>\n",
       "      <td>0.361529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7/20/2010</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.378823</td>\n",
       "      <td>0.378823</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.378823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305632</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9/10/2015</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.36228</td>\n",
       "      <td>0.36228</td>\n",
       "      <td>0.36228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.292285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/5/2020</td>\n",
       "      <td>0.36228</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.36228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.36228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.36228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.292285</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  TFIDF_and  TFIDF_are  TFIDF_awareness  TFIDF_campaigns  \\\n",
       "0   1/1/2001    0.00000    0.00000          0.00000          0.00000   \n",
       "1  5/15/2005    0.00000    0.00000          0.00000          0.00000   \n",
       "2  7/20/2010    0.00000    0.00000          0.00000          0.00000   \n",
       "3  9/10/2015    0.00000    0.36228          0.36228          0.36228   \n",
       "4  12/5/2020    0.36228    0.00000          0.00000          0.00000   \n",
       "\n",
       "   TFIDF_community  TFIDF_cover  TFIDF_data  TFIDF_deforestation  TFIDF_down  \\\n",
       "0         0.000000     0.000000     0.00000             0.000000    0.000000   \n",
       "1         0.000000     0.000000     0.00000             0.361529    0.361529   \n",
       "2         0.378823     0.378823     0.00000             0.000000    0.000000   \n",
       "3         0.000000     0.000000     0.00000             0.000000    0.000000   \n",
       "4         0.000000     0.000000     0.36228             0.000000    0.000000   \n",
       "\n",
       "   ...  TFIDF_rainfall  TFIDF_recent  TFIDF_reduced  TFIDF_region  \\\n",
       "0  ...        0.388684       0.00000       0.000000      0.000000   \n",
       "1  ...        0.000000       0.00000       0.000000      0.361529   \n",
       "2  ...        0.000000       0.00000       0.378823      0.000000   \n",
       "3  ...        0.000000       0.00000       0.000000      0.000000   \n",
       "4  ...        0.000000       0.36228       0.000000      0.000000   \n",
       "\n",
       "   TFIDF_shows  TFIDF_slowing  TFIDF_the  TFIDF_to  TFIDF_tree  TFIDF_western  \n",
       "0      0.00000       0.000000   0.000000  0.313587    0.000000       0.000000  \n",
       "1      0.00000       0.361529   0.361529  0.000000    0.000000       0.361529  \n",
       "2      0.00000       0.000000   0.000000  0.000000    0.305632       0.000000  \n",
       "3      0.00000       0.000000   0.000000  0.292285    0.000000       0.000000  \n",
       "4      0.36228       0.000000   0.000000  0.000000    0.292285       0.000000  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Join tokens back to text (since TF-IDF expects strings)\n",
    "text_df['Processed_Text'] = text_df['Clean_Text']\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=100)  # Limit to top 100 terms to avoid too many features\n",
    "\n",
    "# Fit and transform\n",
    "tfidf_matrix = tfidf.fit_transform(text_df['Processed_Text'])\n",
    "\n",
    "# Convert to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=[f'TFIDF_{word}' for word in tfidf.get_feature_names_out()])\n",
    "\n",
    "# Combine with the original dataframe (keeping Date)\n",
    "final_text_features = pd.concat([text_df[['Date']], tfidf_df], axis=1)\n",
    "\n",
    "# Preview\n",
    "final_text_features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09ac3819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Sentiment_Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/2001</td>\n",
       "      <td>-0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5/15/2005</td>\n",
       "      <td>-0.077778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7/20/2010</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9/10/2015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/5/2020</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Sentiment_Polarity\n",
       "0   1/1/2001           -0.187500\n",
       "1  5/15/2005           -0.077778\n",
       "2  7/20/2010            0.000000\n",
       "3  9/10/2015            0.000000\n",
       "4  12/5/2020            0.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Function to calculate polarity\n",
    "def get_polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Apply polarity score\n",
    "text_df['Sentiment_Polarity'] = text_df['Clean_Text'].apply(get_polarity)\n",
    "\n",
    "# Preview\n",
    "text_df[['Date', 'Sentiment_Polarity']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66923a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Sentiment_Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>-0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005</td>\n",
       "      <td>-0.077778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Sentiment_Polarity\n",
       "0  2001           -0.187500\n",
       "1  2005           -0.077778\n",
       "2  2010            0.000000\n",
       "3  2015            0.000000\n",
       "4  2020            0.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract year from Date\n",
    "text_df['Year'] = pd.to_datetime(text_df['Date']).dt.year\n",
    "\n",
    "# Group by Year and calculate average sentiment\n",
    "annual_sentiment = text_df.groupby('Year')['Sentiment_Polarity'].mean().reset_index()\n",
    "\n",
    "# Preview\n",
    "annual_sentiment.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52130676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Year', 'scaled_total_tc_loss', 'scaled_tc_loss_lag1',\n",
      "       'scaled_Precipitation'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "features_df = pd.read_csv('final_features.csv')\n",
    "print(features_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c2ca23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after merge:\n",
      "Index(['Year', 'scaled_total_tc_loss', 'scaled_tc_loss_lag1',\n",
      "       'scaled_Precipitation', 'Sentiment_Score'],\n",
      "      dtype='object')\n",
      "\n",
      "--- Model Performance ---\n",
      "Mean Absolute Error (MAE): 0.9608\n",
      "Root Mean Squared Error (RMSE): 1.0265\n",
      "R-squared (R²): 0.4677\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 1: Load the final features and sentiment score\n",
    "# ----------------------------------------\n",
    "\n",
    "# Load feature dataset\n",
    "features_df = pd.read_csv('final_features.csv')\n",
    "\n",
    "# Load sentiment scores\n",
    "annual_sentiment = pd.read_csv('annual_sentiment.csv')\n",
    "\n",
    "# Merge sentiment into features_df\n",
    "features_df = pd.merge(features_df, annual_sentiment, on='Year', how='left')\n",
    "\n",
    "print(\"Columns after merge:\")\n",
    "print(features_df.columns)\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 2: Prepare Features (X) and Target (y)\n",
    "# ----------------------------------------\n",
    "\n",
    "# Define feature columns and target column\n",
    "X = features_df[['scaled_tc_loss_lag1', 'scaled_Precipitation', 'Sentiment_Score']]\n",
    "y = features_df['scaled_total_tc_loss']\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 3: Split the data into Train and Test\n",
    "# ----------------------------------------\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 4: Train Random Forest Regressor\n",
    "# ----------------------------------------\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 5: Make Predictions\n",
    "# ----------------------------------------\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# ----------------------------------------\n",
    "# STEP 6: Calculate Evaluation Metrics\n",
    "# ----------------------------------------\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n--- Model Performance ---\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb635fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year  Sentiment_Score\n",
      "0  2001        -0.187500\n",
      "1  2005        -0.077778\n",
      "2  2010         0.000000\n",
      "3  2015         0.000000\n",
      "4  2020         0.000000\n",
      "Columns after merge:\n",
      "Index(['Year', 'scaled_total_tc_loss', 'scaled_tc_loss_lag1',\n",
      "       'scaled_Precipitation', 'Sentiment_Score_x', 'Sentiment_Score_y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load sentiment scores\n",
    "annual_sentiment = pd.read_csv('annual_sentiment.csv')\n",
    "\n",
    "# Check to confirm it's loaded correctly\n",
    "print(annual_sentiment)\n",
    "\n",
    "# Merge sentiment into features_df\n",
    "features_df = pd.merge(features_df, annual_sentiment, on='Year', how='left')\n",
    "\n",
    "# Confirm the merge\n",
    "print(\"Columns after merge:\")\n",
    "print(features_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b14aa9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Year', 'scaled_total_tc_loss', 'scaled_tc_loss_lag1',\n",
      "       'scaled_Precipitation'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the correct dataset that contains Sentiment_Score\n",
    "features_df = pd.read_csv('final_features.csv')\n",
    "\n",
    "# Check columns to confirm\n",
    "print(features_df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1bcfb0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['total_tc_loss', 'tc_loss_lag1', 'Precipitation', 'Sentiment_Score'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m numeric_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_tc_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtc_loss_lag1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecipitation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment_Score\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m----> 4\u001b[0m scaled_values \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mfeatures_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumeric_features\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      6\u001b[0m scaled_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(scaled_values, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaled_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m numeric_features])\n\u001b[0;32m      7\u001b[0m scaled_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['total_tc_loss', 'tc_loss_lag1', 'Precipitation', 'Sentiment_Score'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "numeric_features = ['total_tc_loss', 'tc_loss_lag1', 'Precipitation', 'Sentiment_Score']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_values = scaler.fit_transform(features_df[numeric_features])\n",
    "\n",
    "scaled_df = pd.DataFrame(scaled_values, columns=[f'scaled_{col}' for col in numeric_features])\n",
    "scaled_df['Year'] = features_df['Year']\n",
    "\n",
    "print(scaled_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9241b7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Year', 'scaled_total_tc_loss', 'scaled_tc_loss_lag1',\n",
      "       'scaled_Precipitation'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(features_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8ebdb03",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Sentiment_Score'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Scale Sentiment_Score separately\u001b[39;00m\n\u001b[0;32m      4\u001b[0m scaler_sent \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m----> 5\u001b[0m features_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaled_Sentiment_Score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m scaler_sent\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mfeatures_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSentiment_Score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Check the final dataframe\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(features_df\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['Sentiment_Score'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale Sentiment_Score separately\n",
    "scaler_sent = StandardScaler()\n",
    "features_df['scaled_Sentiment_Score'] = scaler_sent.fit_transform(features_df[['Sentiment_Score']])\n",
    "\n",
    "# Check the final dataframe\n",
    "print(features_df.columns)\n",
    "features_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
